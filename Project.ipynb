{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed12c57d",
   "metadata": {},
   "source": [
    "# Crime Classification – CS 9548 Project\n",
    "**Goal:** Exploring Machine Learning Techniques for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b771e8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65678ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 1)) (0.3.13)\n",
      "Requirement already satisfied: numpy in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 3)) (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 7)) (2.20.0)\n",
      "Requirement already satisfied: pathlib in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from -r requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from kagglehub->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from kagglehub->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from kagglehub->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from kagglehub->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (6.33.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 7)) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jacki\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm->kagglehub->-r requirements.txt (line 1)) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce415497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf81b5",
   "metadata": {},
   "source": [
    "## Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e1e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\jacki\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"odins0n/ucf-crime-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda1ea8",
   "metadata": {},
   "source": [
    "## Create DataFrame of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23a855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir: C:\\Users\\jacki\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\n",
      "C:\\Users\\jacki\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\\Train\n",
      "C:\\Users\\jacki\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\\Test\n",
      "label\n",
      "NormalVideos     947768\n",
      "Stealing          44802\n",
      "Robbery           41493\n",
      "Burglary          39504\n",
      "Arrest            26397\n",
      "Shoplifting       24835\n",
      "Fighting          24684\n",
      "Arson             24421\n",
      "RoadAccidents     23486\n",
      "Abuse             19076\n",
      "Explosion         18753\n",
      "Vandalism         13626\n",
      "Assault           10360\n",
      "Shooting           7140\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "NormalVideos     64952\n",
      "Burglary          7657\n",
      "Shooting          7630\n",
      "Shoplifting       7623\n",
      "Explosion         6510\n",
      "Arrest            3365\n",
      "Arson             2793\n",
      "RoadAccidents     2663\n",
      "Assault           2657\n",
      "Stealing          1984\n",
      "Fighting          1231\n",
      "Vandalism         1111\n",
      "Robbery            835\n",
      "Abuse              297\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "# Get directories of train and test datasets\n",
    "data_dir = pathlib.Path(path)\n",
    "train_dir = data_dir / \"Train\"\n",
    "test_dir = data_dir / \"Test\"\n",
    "\n",
    "print(\"Data dir:\", data_dir)\n",
    "print(train_dir)\n",
    "print(test_dir)\n",
    "\n",
    "# Function to build dataframe\n",
    "def build_image_df(root_dir):\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    image_paths = list(root_dir.glob(\"*/*.png\"))\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for p in image_paths:\n",
    "        label = p.parent.name \n",
    "        rows.append({\"image\": str(p), \"label\": label})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Build train and test dataframes\n",
    "train = build_image_df(train_dir)\n",
    "test = build_image_df(test_dir)\n",
    "\n",
    "print(train['label'].value_counts())\n",
    "print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c42d6",
   "metadata": {},
   "source": [
    "## Encode Label as Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3422e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 14\n",
      "Class name vs label_idx:\n",
      "0 Abuse\n",
      "1 Arrest\n",
      "2 Arson\n",
      "3 Assault\n",
      "4 Burglary\n",
      "5 Explosion\n",
      "6 Fighting\n",
      "7 NormalVideos\n",
      "8 RoadAccidents\n",
      "9 Robbery\n",
      "10 Shooting\n",
      "11 Shoplifting\n",
      "12 Stealing\n",
      "13 Vandalism\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create and fit label encoder on training labels\n",
    "le = LabelEncoder()\n",
    "train['label_idx'] = le.fit_transform(train['label'])\n",
    "\n",
    "# Apply the same encoding to test labels\n",
    "test['label_idx'] = le.transform(test['label'])\n",
    "\n",
    "# Number of classes and mapping\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Class name vs label_idx:\")\n",
    "for index, cls in enumerate(le.classes_):\n",
    "    print(index, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5971e0e",
   "metadata": {},
   "source": [
    "We can see that the dataset has a large class imbalance. To mitigate this, we can do some under/oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc906b90",
   "metadata": {},
   "source": [
    "## Under/Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ea5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize sizes for over/undersampling\n",
    "TARGET_NORMAL = 45000   # maximum size for NormalVideos undersampling\n",
    "TARGET_OTHER  = 40000   # minimum size for other classes oversampling\n",
    "\n",
    "# Undersample NormalVideos to 50,000\n",
    "normal_mask = train['label'] == \"NormalVideos\"\n",
    "\n",
    "normal_df = train[normal_mask].sample(n=TARGET_NORMAL, random_state=42)\n",
    "\n",
    "# Oversample all other classes\n",
    "\n",
    "# Create balanced_dfs as a list\n",
    "balanced_dfs = [normal_df]\n",
    "\n",
    "for cls in train['label'].unique():\n",
    "    if cls == \"NormalVideos\":\n",
    "        continue\n",
    "    \n",
    "    # Check number of samples for each class\n",
    "    cls_df = train[train['label'] == cls]\n",
    "    n_current = len(cls_df)\n",
    "    \n",
    "    # If there's enough samples, leave as it is\n",
    "    if n_current >= TARGET_OTHER:\n",
    "        balanced_dfs.append(cls_df)\n",
    "\n",
    "    else:\n",
    "        # Oversample with replacement\n",
    "        extra = cls_df.sample(\n",
    "            n=TARGET_OTHER - n_current,\n",
    "            replace=True,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        cls_balanced = pd.concat([cls_df, extra], ignore_index=True)\n",
    "        balanced_dfs.append(cls_balanced)\n",
    "\n",
    "# Put into new balanced training dataframe\n",
    "train_balanced = pd.concat(balanced_dfs).reset_index(drop=True)\n",
    "\n",
    "# Encode labels again\n",
    "train_balanced['label_idx'] = le.transform(train_balanced['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ed6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "NormalVideos     45000\n",
      "Stealing         44802\n",
      "Robbery          41493\n",
      "Arson            40000\n",
      "Arrest           40000\n",
      "Abuse            40000\n",
      "Burglary         40000\n",
      "Assault          40000\n",
      "Fighting         40000\n",
      "Explosion        40000\n",
      "RoadAccidents    40000\n",
      "Shooting         40000\n",
      "Shoplifting      40000\n",
      "Vandalism        40000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e64b9c",
   "metadata": {},
   "source": [
    "## Split Train Into Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b54d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train 20% val\n",
    "train, val = train_test_split(\n",
    "    train_balanced, test_size=0.20, random_state=42, stratify=train_balanced['label_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8515e6",
   "metadata": {},
   "source": [
    "## Set up TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc63326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set constants for tensorflow\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07efb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "def load_image(path, label):\n",
    "    # Read file\n",
    "    img = tf.io.read_file(path)\n",
    "\n",
    "    # Decode png as rgb\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "\n",
    "    # Normalize\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f9b79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to dataset\n",
    "def df_to_dataset(df, shuffle, batch_size):\n",
    "    # Extract image paths and labels\n",
    "    paths = df['image'].values\n",
    "    labels = df['label_idx'].values\n",
    "\n",
    "    # Create tensorflow dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    # Load and preprocess image\n",
    "    ds = ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Shuffle dataset for training\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "\n",
    "    # Batch and prefetch dataset\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97249322",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_to_dataset(train, True, BATCH_SIZE)\n",
    "val = df_to_dataset(val, True, BATCH_SIZE)\n",
    "test = df_to_dataset(test, False, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209c3b3",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91e0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "def add_augmentation(images, labels):\n",
    "    images = data_augmentation(images, training=True)\n",
    "    return images, labels\n",
    "\n",
    "train = train.map(add_augmentation, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4118e",
   "metadata": {},
   "source": [
    "## Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35afde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(64, 64, 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6515fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,598</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m2,097,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │         \u001b[38;5;34m3,598\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,194,254</span> (8.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,194,254\u001b[0m (8.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,194,254</span> (8.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,194,254\u001b[0m (8.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12432f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    x=train,\n",
    "    validation_data=val,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed12c57d",
   "metadata": {},
   "source": [
    "# Crime Classification â€“ CS 9548 Project\n",
    "**Goal:** Exploring Machine Learning Techniques for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b771e8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65678ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (0.3.13)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 7)) (2.20.0)\n",
      "Requirement already satisfied: pathlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from kagglehub->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kagglehub->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (6.33.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow->-r requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->-r requirements.txt (line 7)) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 7)) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->kagglehub->-r requirements.txt (line 1)) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce415497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf81b5",
   "metadata": {},
   "source": [
    "## Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e1e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\admin\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"odins0n/ucf-crime-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda1ea8",
   "metadata": {},
   "source": [
    "## Create DataFrame of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23a855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir: C:\\Users\\admin\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\n",
      "C:\\Users\\admin\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\\Train\n",
      "C:\\Users\\admin\\.cache\\kagglehub\\datasets\\odins0n\\ucf-crime-dataset\\versions\\1\\Test\n",
      "label\n",
      "NormalVideos     947768\n",
      "Stealing          44802\n",
      "Robbery           41493\n",
      "Burglary          39504\n",
      "Arrest            26397\n",
      "Shoplifting       24835\n",
      "Fighting          24684\n",
      "Arson             24421\n",
      "RoadAccidents     23486\n",
      "Abuse             19076\n",
      "Explosion         18753\n",
      "Vandalism         13626\n",
      "Assault           10360\n",
      "Shooting           7140\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "NormalVideos     64952\n",
      "Burglary          7657\n",
      "Shooting          7630\n",
      "Shoplifting       7623\n",
      "Explosion         6510\n",
      "Arrest            3365\n",
      "Arson             2793\n",
      "RoadAccidents     2663\n",
      "Assault           2657\n",
      "Stealing          1984\n",
      "Fighting          1231\n",
      "Vandalism         1111\n",
      "Robbery            835\n",
      "Abuse              297\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "# Get directories of train and test datasets\n",
    "data_dir = pathlib.Path(path)\n",
    "train_dir = data_dir / \"Train\"\n",
    "test_dir = data_dir / \"Test\"\n",
    "\n",
    "print(\"Data dir:\", data_dir)\n",
    "print(train_dir)\n",
    "print(test_dir)\n",
    "\n",
    "# Function to build dataframe\n",
    "def build_image_df(root_dir):\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    image_paths = list(root_dir.glob(\"*/*.png\"))\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for p in image_paths:\n",
    "        label = p.parent.name \n",
    "        rows.append({\"image\": str(p), \"label\": label})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Build train and test dataframes\n",
    "train = build_image_df(train_dir)\n",
    "test = build_image_df(test_dir)\n",
    "\n",
    "print(train['label'].value_counts())\n",
    "print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c42d6",
   "metadata": {},
   "source": [
    "## Encode Label as Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3422e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 14\n",
      "Class name vs label_idx:\n",
      "0 Abuse\n",
      "1 Arrest\n",
      "2 Arson\n",
      "3 Assault\n",
      "4 Burglary\n",
      "5 Explosion\n",
      "6 Fighting\n",
      "7 NormalVideos\n",
      "8 RoadAccidents\n",
      "9 Robbery\n",
      "10 Shooting\n",
      "11 Shoplifting\n",
      "12 Stealing\n",
      "13 Vandalism\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create and fit label encoder on training labels\n",
    "le = LabelEncoder()\n",
    "train['label_idx'] = le.fit_transform(train['label'])\n",
    "\n",
    "# Apply the same encoding to test labels\n",
    "test['label_idx'] = le.transform(test['label'])\n",
    "\n",
    "# Number of classes and mapping\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Class name vs label_idx:\")\n",
    "for index, cls in enumerate(le.classes_):\n",
    "    print(index, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5971e0e",
   "metadata": {},
   "source": [
    "We can see that the dataset has a large class imbalance. To mitigate this, we can do some under/oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc906b90",
   "metadata": {},
   "source": [
    "## Under/Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ea5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TARGET = 50000\n",
    "EXEMPT_CLASS = \"Shooting\"\n",
    "\n",
    "def balance_dataset(df):\n",
    "    balanced_parts = []\n",
    "\n",
    "    for cls, group in df.groupby(\"label\"):\n",
    "        if cls == EXEMPT_CLASS:\n",
    "            # Oversample shooting to 10k\n",
    "            if len(group) < TARGET:\n",
    "                group = group.sample(\n",
    "                    n=TARGET,\n",
    "                    replace=True,\n",
    "                    random_state=42\n",
    "                )\n",
    "            balanced_parts.append(group)\n",
    "        else:\n",
    "            # Undersample everything else to 10k\n",
    "            balanced_parts.append(group.sample(\n",
    "                n=min(TARGET, len(group)),\n",
    "                replace=False,\n",
    "                random_state=42\n",
    "            ))\n",
    "\n",
    "    return pd.concat(balanced_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_balanced = balance_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ed6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "NormalVideos     50000\n",
      "Shooting         50000\n",
      "Stealing         44802\n",
      "Robbery          41493\n",
      "Burglary         39504\n",
      "Arrest           26397\n",
      "Shoplifting      24835\n",
      "Fighting         24684\n",
      "Arson            24421\n",
      "RoadAccidents    23486\n",
      "Abuse            19076\n",
      "Explosion        18753\n",
      "Vandalism        13626\n",
      "Assault          10360\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e64b9c",
   "metadata": {},
   "source": [
    "## Split Train Into Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b54d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train 20% val\n",
    "train, val = train_test_split(\n",
    "    train_balanced, test_size=0.20, random_state=42, stratify=train_balanced['label_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8515e6",
   "metadata": {},
   "source": [
    "## Set up PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc63326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Set constants for PyTorch\n",
    "IMG_SIZE = (48, 48)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc30e6",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6011a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation to training dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(contrast=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Simple transforms to val/test dataset\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e9538",
   "metadata": {},
   "source": [
    "## Create Pipeline For PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5208926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['image']\n",
    "        label = int(row['label_idx'])\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "        # Transform if needed (might not need for val/test)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d578ed",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84a4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert to CrimeDataset classes\n",
    "train_dataset = CrimeDataset(train, train_transform)\n",
    "val_dataset = CrimeDataset(val, eval_transform)\n",
    "test_dataset = CrimeDataset(test, eval_transform)\n",
    "\n",
    "# Transform to DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd9865",
   "metadata": {},
   "source": [
    "## Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42516277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # --- Block 1 ---\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.drop1 = nn.Dropout(0.4)    \n",
    "\n",
    "        # --- Block 2 ---\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.drop2 = nn.Dropout(0.4)    \n",
    "\n",
    "        # --- Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.drop3 = nn.Dropout(0.4)   \n",
    "\n",
    "        # --- Block 4 ---\n",
    "        self.conv4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.drop4 = nn.Dropout(0.4)  \n",
    "\n",
    "        self.flatten_dim = 512 * 3 * 3\n",
    "\n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 512)\n",
    "        self.drop_fc1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.drop_fc2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Block 1 ---\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # --- Block 2 ---\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_fc1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_fc2(x)\n",
    "        x = self.fc3(x)          \n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = Net(num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7202c",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b4d563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73940376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4b62813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.4724, Train Acc: 0.5359 | Val Loss: 0.9532, Val Acc: 0.7578\n",
      "Epoch 2/10 | Train Loss: 0.9833, Train Acc: 0.6989 | Val Loss: 0.8549, Val Acc: 0.8033\n",
      "Epoch 3/10 | Train Loss: 0.8225, Train Acc: 0.7506 | Val Loss: 0.6152, Val Acc: 0.8539\n",
      "Epoch 4/10 | Train Loss: 0.7251, Train Acc: 0.7824 | Val Loss: 0.4581, Val Acc: 0.8957\n",
      "Epoch 5/10 | Train Loss: 0.6630, Train Acc: 0.8020 | Val Loss: 0.3705, Val Acc: 0.9141\n",
      "Epoch 6/10 | Train Loss: 0.6163, Train Acc: 0.8168 | Val Loss: 0.4444, Val Acc: 0.9023\n",
      "Epoch 7/10 | Train Loss: 0.5782, Train Acc: 0.8294 | Val Loss: 0.3606, Val Acc: 0.9223\n",
      "Epoch 8/10 | Train Loss: 0.5592, Train Acc: 0.8357 | Val Loss: 0.4629, Val Acc: 0.8975\n",
      "Epoch 9/10 | Train Loss: 0.5319, Train Acc: 0.8438 | Val Loss: 0.3043, Val Acc: 0.9265\n",
      "Epoch 10/10 | Train Loss: 0.5139, Train Acc: 0.8502 | Val Loss: 0.5879, Val Acc: 0.8486\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = test(model, val_loader, criterion, device)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2bd084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2808, Test Acc: 0.4881\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
